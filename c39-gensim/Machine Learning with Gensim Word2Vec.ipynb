{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python\n",
    "\n",
    "## Machine Learning Concepts\n",
    "\n",
    "A very brief introduction to machine learning concepts - by no means comprehensive.\n",
    "\n",
    "### Data Representation\n",
    "\n",
    "Data has to be organized and quantified so the learning algorithm can operate on it. For example, for an algorithm to to recognize text from an image, the image has to be analyzed to determine the outlines of characters, and characters have to be analyzed to determine words (a little more detail at https://www.quora.com/How-does-the-Tesseract-API-for-OCR-work). Or, for the example we'll look at today, a learning algorithm can learn to recognize documents by using a dictionary of words, and counting the number of times a word appears in paragraphs in the documents.\n",
    "\n",
    "### Bag of Words (BoW) Models\n",
    "\n",
    "A simple and common way of representing documents is to create a list of words found in the documents, and then list the number of times each word is found in the document.\n",
    "\n",
    "Using two simple documents from https://en.wikipedia.org/wiki/Bag-of-words_model:\n",
    "\n",
    "1. `John likes to watch movies. Mary likes movies too.`\n",
    "1. `John also likes to watch football games.`\n",
    "\n",
    "The documents can be represented by these bags of words:\n",
    "\n",
    "1. `{\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1}`\n",
    "1. `{\"John\":1,\"also\":1,\"likes\":1,\"to\":1,\"watch\":1,\"football\":1,\"games\":1}`\n",
    "\n",
    "After analyzing a set of documents, a list of all the distinct words can be used to create an array of the number of times each known word appears in each document. So, for these documents, we'd use this list of words:\n",
    "\n",
    "```\n",
    "[\"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\",\"too\",\"also\",\"football\",\"games\"]\n",
    "```\n",
    "\n",
    "Then, we can represent each document by a list of numbers of times each word appears:\n",
    "\n",
    "1. `[1, 2, 1, 1, 2, 1, 1, 0, 0, 0]`\n",
    "1. `[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]`\n",
    "\n",
    "#### Context\n",
    "\n",
    "Bag-of-words representations are obviously limited: there is no context between words. \"Rain fall\" is the same as \"fall rain\", which is obviously different to us humans. We can extend the vocabulary to phrases by using N-grams: grouping words, like \"John likes\", \"likes to\", and \"to watch\", to gain context between individual words.\n",
    "\n",
    "#### Junk Words (Stop words)\n",
    "\n",
    "Some words occur with such frequency that they not helpful. 'A', 'an', and 'the' are mostly useless to machine learning algorithms. A textual machine-learning algorithm will typically ignore them.\n",
    "\n",
    "\n",
    "### Learning (Training) and Testing\n",
    "\n",
    "Two general steps: during learning, an algorithm is trained using input data. During testing, the learned concept is checked aginst test data to determine the learned concept's effectiveness. A common approach is to randomly select 80% of input documents for training, perform the training operation, and then test the learned concept on the other 20% of documents to determine efficacy.\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "Training data specifies output (results) - each input has a \"label\" that the algorithm learns.\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "Training data lacks specified results, so the learning algorithm has to arbitrarily group the data.\n",
    "\n",
    "### Semi-supervised Learning\n",
    "\n",
    "Some training data includes specific labels, and the algorithm has to generalize its learning from those labels.\n",
    "\n",
    "### Feedback Learning\n",
    "\n",
    "An oracle observes the learning algorithm's outputs and gives feedback to the algorithm so it can improve its learned concept.\n",
    "\n",
    "\n",
    "## Gensim\n",
    "\n",
    "The Python gensim module provides a number of machine learning algorithms that work with text:\n",
    "* Word2Vec\n",
    "* Doc2Vec\n",
    "* FastText\n",
    "* Latent Semantic Analysis (LSI, LSA, see LsiModel)\n",
    "* Latent Dirichlet Allocation (LDA, see LdaModel)\n",
    "\n",
    "These algorithms learn to recognize text using statistical patterns. These algorithms are unsupervised, which means no human input is necessary â€“ you only need a corpus (large set) of document files.\n",
    "\n",
    "An explanation of how word2vec works: http://www.deeplearningweekly.com/blog/demystifying-word2vec \n",
    "\n",
    "> You can also generalize inputs to algorithms like these to work with non-text data. If you can construct a \"dictionary\" or \"vocabulary\" of possible inputs over your data (like a set of words), the algorithms can be applied to your data just like it would operate on plain text.\n",
    "\n",
    "You'll need to use pip to install gensim. Also install nltk for easy access to Project Gutenberg (public domain) documents. In the Gerdin lab:\n",
    "```\n",
    "pip install gensim\n",
    "pip install nltk\n",
    "```\n",
    "Or, on your own Windows laptop:\n",
    "```\n",
    "pip install --user gensim\n",
    "pip install --user nltk\n",
    "```\n",
    "\n",
    "There are wheel modules containing compiled code for the gensim submodules. Pip should be able to download and install them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the Gutenberg corpus from nltk, the nltk punctuation-aware tokenizer, read and parse Macbeth, process it using Word2Vec, and look for some common nearby words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/ghelmer/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ghelmer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import gensim\n",
    "from gensim import models, similarities\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "macbeth_sentence = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "macbeth_model = gensim.models.Word2Vec(macbeth_sentence, min_count=1, size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghelmer/Library/Python/3.6/lib/python/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('and', 0.9994088411331177),\n",
       " ('of', 0.9994024038314819),\n",
       " (';', 0.9993935227394104),\n",
       " ('d', 0.9993923306465149),\n",
       " ('my', 0.9993640184402466),\n",
       " ('me', 0.9993475675582886),\n",
       " ('with', 0.9993475079536438),\n",
       " ('to', 0.9993457198143005),\n",
       " ('The', 0.9993445873260498),\n",
       " ('be', 0.9993438720703125)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_model.wv.most_similar('King')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the greatest results -- we need more data than just one book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something similar with Milton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "milton_sentence = gutenberg.sents('milton-paradise.txt')\n",
    "milton_model = gensim.models.Word2Vec(milton_sentence, min_count=1, size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghelmer/Library/Python/3.6/lib/python/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('risen', 0.9618339538574219),\n",
       " ('provoke', 0.9600409865379333),\n",
       " ('stole', 0.959398627281189),\n",
       " ('courage', 0.9587253332138062),\n",
       " ('dove', 0.9584486484527588),\n",
       " ('feed', 0.9582011103630066),\n",
       " ('vouchsafed', 0.9578553438186646),\n",
       " ('endured', 0.9577709436416626),\n",
       " ('Sea', 0.9576237797737122),\n",
       " ('fabled', 0.9570697546005249)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "milton_model.wv.most_similar('Mercy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghelmer/Library/Python/3.6/lib/python/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rash', 0.9622334241867065),\n",
       " ('Companion', 0.9583115577697754),\n",
       " ('oak', 0.9513239860534668),\n",
       " ('growth', 0.9511553049087524),\n",
       " ('listen', 0.9502556324005127),\n",
       " ('crushed', 0.9500308632850647),\n",
       " ('tumultuous', 0.9494795799255371),\n",
       " ('loth', 0.94921875),\n",
       " ('excels', 0.9474302530288696),\n",
       " ('defiance', 0.9470477104187012)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lookup words similar to Evil\n",
    "mm_evil = ['Evil']\n",
    "milton_model.wv.most_similar(positive=mm_evil, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not great results. We would need a larger corpus to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Visualization\n",
    "\n",
    "Handy visualizations from Word2Vec data:\n",
    "\n",
    "https://github.com/anvaka/word2vec-graph \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
